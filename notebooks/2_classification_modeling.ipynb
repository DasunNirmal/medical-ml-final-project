{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00017a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_recall_fscore_support)\n",
    "\n",
    "# For model persistence\n",
    "import joblib\n",
    "\n",
    "# For loading dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c82e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING MEDICAL DATASET\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset loaded successfully!\n",
      "\n",
      "Training set: 1724 samples\n",
      "Validation set: 370 samples\n",
      "Test set: 370 samples\n",
      "Total unique specialties: 13\n",
      "\n",
      "Class distribution in training set:\n",
      "medical_specialty\n",
      "Cardiovascular / Pulmonary    526\n",
      "Orthopedic                    289\n",
      "Neurology                     187\n",
      "Gastroenterology              152\n",
      "Obstetrics / Gynecology       126\n",
      "Hematology - Oncology          86\n",
      "Neurosurgery                   76\n",
      "ENT - Otolaryngology           53\n",
      "Pediatrics - Neonatal          51\n",
      "Psychiatry / Psychology        49\n",
      "Nephrology                     45\n",
      "Ophthalmology                  45\n",
      "Radiology                      39\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MEDICAL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"hpe-ai/medical-cases-classification-tutorial\")\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "val_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Prepare feature (X) and target (y) variables\n",
    "X_train = train_df['transcription']\n",
    "y_train = train_df['medical_specialty']\n",
    "\n",
    "X_val = val_df['transcription']\n",
    "y_val = val_df['medical_specialty']\n",
    "\n",
    "X_test = test_df['transcription']\n",
    "y_test = test_df['medical_specialty']\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Total unique specialties: {y_train.nunique()}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47bb66",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "<!-- Purpose: Load the medical dataset and prepare train/validation/test splits\n",
    "Reusing the same data preparation from Notebook 1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001346ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING BASELINE MODEL PIPELINE\n",
      "======================================================================\n",
      "\n",
      "✅ Baseline pipeline created successfully!\n",
      "\n",
      "Pipeline steps:\n",
      "  1. TfidfVectorizer: Converts text to numerical features\n",
      "  2. LogisticRegression: Trains multinomial classification model\n",
      "\n",
      "Model configuration:\n",
      "  • Multi-class strategy: multinomial\n",
      "  • Solver: lbfgs\n",
      "  • Max iterations: 1000\n",
      "  • TF-IDF max features: 5000\n",
      "  • N-gram range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING BASELINE MODEL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create TfidfVectorizer (same configuration as Notebook 1)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents='ascii'\n",
    ")\n",
    "\n",
    "# Create Softmax Regression classifier (Multinomial Logistic Regression)\n",
    "classifier = LogisticRegression(\n",
    "    multi_class='multinomial',    # Use softmax for multi-class classification\n",
    "    solver='lbfgs',                # Optimizer algorithm\n",
    "    max_iter=1000,                 # Maximum iterations for convergence\n",
    "    random_state=42,               # For reproducibility\n",
    "    n_jobs=-1                      # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Combine into a single pipeline\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),         # Step 1: Convert text to TF-IDF features\n",
    "    ('classifier', classifier)     # Step 2: Train logistic regression model\n",
    "])\n",
    "\n",
    "print(\"\\n✅ Baseline pipeline created successfully!\")\n",
    "print(\"\\nPipeline steps:\")\n",
    "print(\"  1. TfidfVectorizer: Converts text to numerical features\")\n",
    "print(\"  2. LogisticRegression: Trains multinomial classification model\")\n",
    "\n",
    "print(\"\\nModel configuration:\")\n",
    "print(f\"  • Multi-class strategy: {classifier.multi_class}\")\n",
    "print(f\"  • Solver: {classifier.solver}\")\n",
    "print(f\"  • Max iterations: {classifier.max_iter}\")\n",
    "print(f\"  • TF-IDF max features: {vectorizer.max_features}\")\n",
    "print(f\"  • N-gram range: {vectorizer.ngram_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b116f",
   "metadata": {},
   "source": [
    "## Create Baseline Model Pipeline\n",
    "<!-- Purpose: Build a complete pipeline with TfidfVectorizer + Logistic Regression\n",
    "This creates a baseline model before hyperparameter tuning -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b024eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING BASELINE MODEL\n",
      "======================================================================\n",
      "\n",
      "Training model on 1,720 documents...\n",
      "This may take 1-2 minutes...\n",
      "\n",
      "✅ Model training complete!\n",
      "\n",
      "Vocabulary size: 5000 unique terms extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTraining model on 1,720 documents...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Train the pipeline on training data\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Model training complete!\")\n",
    "\n",
    "# Get vocabulary size after fitting\n",
    "vocab_size = len(baseline_pipeline.named_steps['tfidf'].vocabulary_)\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique terms extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c986d1",
   "metadata": {},
   "source": [
    "## Train Baseline Model\n",
    "<!-- Purpose: Fit the baseline model on training data\n",
    "Establishes initial performance before optimization -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
