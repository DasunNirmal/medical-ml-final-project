{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00017a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully!\n",
      "Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_recall_fscore_support)\n",
    "\n",
    "# For model persistence\n",
    "import joblib\n",
    "\n",
    "# For loading dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c82e3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOADING MEDICAL DATASET\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset loaded successfully!\n",
      "\n",
      "Training set: 1724 samples\n",
      "Validation set: 370 samples\n",
      "Test set: 370 samples\n",
      "Total unique specialties: 13\n",
      "\n",
      "Class distribution in training set:\n",
      "medical_specialty\n",
      "Cardiovascular / Pulmonary    526\n",
      "Orthopedic                    289\n",
      "Neurology                     187\n",
      "Gastroenterology              152\n",
      "Obstetrics / Gynecology       126\n",
      "Hematology - Oncology          86\n",
      "Neurosurgery                   76\n",
      "ENT - Otolaryngology           53\n",
      "Pediatrics - Neonatal          51\n",
      "Psychiatry / Psychology        49\n",
      "Nephrology                     45\n",
      "Ophthalmology                  45\n",
      "Radiology                      39\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING MEDICAL DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"hpe-ai/medical-cases-classification-tutorial\")\n",
    "\n",
    "# Convert to pandas DataFrames\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "val_df = pd.DataFrame(dataset['validation'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "# Prepare feature (X) and target (y) variables\n",
    "X_train = train_df['transcription']\n",
    "y_train = train_df['medical_specialty']\n",
    "\n",
    "X_val = val_df['transcription']\n",
    "y_val = val_df['medical_specialty']\n",
    "\n",
    "X_test = test_df['transcription']\n",
    "y_test = test_df['medical_specialty']\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded successfully!\")\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Total unique specialties: {y_train.nunique()}\")\n",
    "\n",
    "# Display class distribution\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47bb66",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "<!-- Purpose: Load the medical dataset and prepare train/validation/test splits\n",
    "Reusing the same data preparation from Notebook 1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001346ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING BASELINE MODEL PIPELINE\n",
      "======================================================================\n",
      "\n",
      "✅ Baseline pipeline created successfully!\n",
      "\n",
      "Pipeline steps:\n",
      "  1. TfidfVectorizer: Converts text to numerical features\n",
      "  2. LogisticRegression: Trains multinomial classification model\n",
      "\n",
      "Model configuration:\n",
      "  • Multi-class strategy: multinomial\n",
      "  • Solver: lbfgs\n",
      "  • Max iterations: 1000\n",
      "  • TF-IDF max features: 5000\n",
      "  • N-gram range: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING BASELINE MODEL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create TfidfVectorizer (same configuration as Notebook 1)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    strip_accents='ascii'\n",
    ")\n",
    "\n",
    "# Create Softmax Regression classifier (Multinomial Logistic Regression)\n",
    "classifier = LogisticRegression(\n",
    "    multi_class='multinomial',    # Use softmax for multi-class classification\n",
    "    solver='lbfgs',                # Optimizer algorithm\n",
    "    max_iter=1000,                 # Maximum iterations for convergence\n",
    "    random_state=42,               # For reproducibility\n",
    "    n_jobs=-1                      # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Combine into a single pipeline\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),         # Step 1: Convert text to TF-IDF features\n",
    "    ('classifier', classifier)     # Step 2: Train logistic regression model\n",
    "])\n",
    "\n",
    "print(\"\\n✅ Baseline pipeline created successfully!\")\n",
    "print(\"\\nPipeline steps:\")\n",
    "print(\"  1. TfidfVectorizer: Converts text to numerical features\")\n",
    "print(\"  2. LogisticRegression: Trains multinomial classification model\")\n",
    "\n",
    "print(\"\\nModel configuration:\")\n",
    "print(f\"  • Multi-class strategy: {classifier.multi_class}\")\n",
    "print(f\"  • Solver: {classifier.solver}\")\n",
    "print(f\"  • Max iterations: {classifier.max_iter}\")\n",
    "print(f\"  • TF-IDF max features: {vectorizer.max_features}\")\n",
    "print(f\"  • N-gram range: {vectorizer.ngram_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b116f",
   "metadata": {},
   "source": [
    "## Create Baseline Model Pipeline\n",
    "<!-- Purpose: Build a complete pipeline with TfidfVectorizer + Logistic Regression\n",
    "This creates a baseline model before hyperparameter tuning -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b024eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING BASELINE MODEL\n",
      "======================================================================\n",
      "\n",
      "Training model on 1,720 documents...\n",
      "This may take 1-2 minutes...\n",
      "\n",
      "✅ Model training complete!\n",
      "\n",
      "Vocabulary size: 5000 unique terms extracted\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nTraining model on 1,720 documents...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Train the pipeline on training data\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Model training complete!\")\n",
    "\n",
    "# Get vocabulary size after fitting\n",
    "vocab_size = len(baseline_pipeline.named_steps['tfidf'].vocabulary_)\n",
    "print(f\"\\nVocabulary size: {vocab_size} unique terms extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c986d1",
   "metadata": {},
   "source": [
    "## Train Baseline Model\n",
    "<!-- Purpose: Fit the baseline model on training data\n",
    "Establishes initial performance before optimization -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbc372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE MODEL - TRAINING SET PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Training Accuracy: 0.8730 (87.30%)\n",
      "✅ Training accuracy in healthy range (70-95%)\n",
      "\n",
      "Classification Report (Training Set):\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "Cardiovascular / Pulmonary       0.90      0.99      0.94       526\n",
      "      ENT - Otolaryngology       1.00      0.83      0.91        53\n",
      "          Gastroenterology       0.91      0.97      0.94       152\n",
      "     Hematology - Oncology       0.88      0.73      0.80        86\n",
      "                Nephrology       1.00      0.53      0.70        45\n",
      "                 Neurology       0.77      0.83      0.80       187\n",
      "              Neurosurgery       0.76      0.42      0.54        76\n",
      "   Obstetrics / Gynecology       0.94      0.95      0.95       126\n",
      "             Ophthalmology       1.00      0.89      0.94        45\n",
      "                Orthopedic       0.81      0.95      0.87       289\n",
      "     Pediatrics - Neonatal       0.93      0.75      0.83        51\n",
      "   Psychiatry / Psychology       0.89      0.86      0.88        49\n",
      "                 Radiology       0.67      0.05      0.10        39\n",
      "\n",
      "                  accuracy                           0.87      1724\n",
      "                 macro avg       0.88      0.75      0.78      1724\n",
      "              weighted avg       0.87      0.87      0.86      1724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL - TRAINING SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions on training data\n",
    "y_train_pred = baseline_pipeline.predict(X_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Check for overfitting indicators\n",
    "if train_accuracy > 0.95:\n",
    "    print(\"⚠️  High training accuracy (>95%) - monitor for overfitting\")\n",
    "elif train_accuracy < 0.70:\n",
    "    print(\"⚠️  Low training accuracy (<70%) - model may be underfitting\")\n",
    "else:\n",
    "    print(\"✅ Training accuracy in healthy range (70-95%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3dd2b",
   "metadata": {},
   "source": [
    "## Baseline Model - Training Set Performance\n",
    "<!-- Purpose: Evaluate model performance on training data\n",
    "Helps identify if model is learning patterns vs. memorizing -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d933efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BASELINE MODEL - VALIDATION SET PERFORMANCE\n",
      "======================================================================\n",
      "\n",
      "Validation Accuracy: 0.7486 (74.86%)\n",
      "\n",
      "Accuracy Gap (Train - Val): 0.1243\n",
      "⚠️  Large gap (>10%) suggests overfitting\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL - VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Make predictions on validation data\n",
    "y_val_pred = baseline_pipeline.predict(X_val)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Compare training vs validation accuracy\n",
    "accuracy_gap = train_accuracy - val_accuracy\n",
    "print(f\"\\nAccuracy Gap (Train - Val): {accuracy_gap:.4f}\")\n",
    "\n",
    "if accuracy_gap > 0.10:\n",
    "    print(\"⚠️  Large gap (>10%) suggests overfitting\")\n",
    "elif accuracy_gap < 0:\n",
    "    print(\"⚠️  Validation accuracy higher than training - unusual, check data\")\n",
    "else:\n",
    "    print(\"✅ Reasonable generalization gap (<10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb855f4",
   "metadata": {},
   "source": [
    "## Baseline Model - Validation Set Performance\n",
    "<!-- Purpose: Evaluate model on unseen validation data\n",
    "More realistic measure of model performance -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf09a062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CROSS-VALIDATION EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Performing 5-fold cross-validation on training data...\n",
      "This may take 3-5 minutes...\n",
      "\n",
      "✅ Cross-validation complete!\n",
      "\n",
      "Cross-Validation Scores (5 folds):\n",
      "----------------------------------------------------------------------\n",
      "  Fold 1: 0.7768 (77.68%)\n",
      "  Fold 2: 0.7652 (76.52%)\n",
      "  Fold 3: 0.7333 (73.33%)\n",
      "  Fold 4: 0.7565 (75.65%)\n",
      "  Fold 5: 0.7558 (75.58%)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Mean CV Accuracy: 0.7575 (75.75%)\n",
      "Std Dev: 0.0143 (±1.43%)\n",
      "95% Confidence Interval: 0.7575 ± 0.0280\n",
      "\n",
      "✅ Low variance across folds - stable model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-VALIDATION EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPerforming 5-fold cross-validation on training data...\")\n",
    "print(\"This may take 3-5 minutes...\\n\")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    baseline_pipeline, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    cv=5,                    # 5 folds\n",
    "    scoring='accuracy',      # Metric to use\n",
    "    n_jobs=-1               # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"✅ Cross-validation complete!\")\n",
    "print(\"\\nCross-Validation Scores (5 folds):\")\n",
    "print(\"-\"*70)\n",
    "for i, score in enumerate(cv_scores, 1):\n",
    "    print(f\"  Fold {i}: {score:.4f} ({score*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
    "print(f\"Std Dev: {cv_scores.std():.4f} (±{cv_scores.std()*100:.2f}%)\")\n",
    "print(f\"95% Confidence Interval: {cv_scores.mean():.4f} ± {1.96*cv_scores.std():.4f}\")\n",
    "\n",
    "# Interpret results\n",
    "if cv_scores.std() < 0.02:\n",
    "    print(\"\\n✅ Low variance across folds - stable model\")\n",
    "else:\n",
    "    print(\"\\n⚠️  High variance across folds - consider more data or regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a936073",
   "metadata": {},
   "source": [
    "## Cross-Validation Evaluation\n",
    "<!-- Purpose: Get robust performance estimate using 5-fold cross-validation\n",
    "Reduces variance in performance estimation -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
